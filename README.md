# RefereeSim: AI Reviewer Evaluation Platform

## Overview

RefereeSim evaluates AI models' ability to detect errors in research papers using **REAL API calls**. The system generates synthetic papers with controlled errors and tests 11 working AI models from Cohere, Gemini, and Hyperbolic (NO OpenAI required).

## Features

âœ… **Real API Integration** - No simulations, actual API calls to Cohere, Gemini, and Hyperbolic  
ğŸ“Š **Confusion Matrices** - Terminal output and saved visualizations  
ğŸ¤– **11 Working AI Models** - Cohere, Gemini, Meta-Llama, DeepSeek (NO OpenAI required)  
ğŸ“ˆ **Comprehensive Reports** - HTML reports, CSV data, performance plots  
ğŸ¯ **Controlled Evaluation** - Synthetic papers with known ground-truth errors  

## Quick Start

### Secure Setup (Environment Variables)
1. Download `run_refereesim.py`
2. Set environment variables:
   ```bash
   export COHERE_API_KEY=your_cohere_key
   export HYPERBOLIC_API_KEY=your_hyperbolic_key  
   export GENAI_API_KEY=your_gemini_key
   ```
3. Install dependencies: `pip install -r requirements.txt`
4. Run: `python run_refereesim.py`

## API Keys Required

- **Cohere**: Get from https://dashboard.cohere.ai/api-keys
- **Hyperbolic**: Get from https://hyperbolic.xyz/
- **Gemini**: Get from https://ai.google.dev/gemini-api/docs/api-key

## Output

The system generates:
- **Terminal Output**: Real-time confusion matrices for each model
- **HTML Report**: Comprehensive analysis with visualizations
- **CSV Files**: Raw performance data
- **Confusion Matrix Images**: Saved as PNG files
- **Performance Plots**: Model comparison charts

## Models Tested (11 Working Models)

### Cohere Models (2)
- command-a-03-2025
- command-r

### Gemini Models (2)  
- gemini-2.5-flash
- gemini-2.5-pro

### Hyperbolic Models (7)
- openai/gpt-oss-120b
- meta-llama/Meta-Llama-3.1-405B-Instruct
- meta-llama/Meta-Llama-3.1-70B-Instruct
- meta-llama/Meta-Llama-3.1-8B-Instruct
- deepseek-ai/DeepSeek-R1-0528
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-V3

## Project Structure

```
RefereeSim_Download/
â”œâ”€â”€ run_refereesim.py           # Secure main file (environment variables)
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ refereesim/                 # Core package
â”‚   â”œâ”€â”€ generators/             # Paper generation
â”‚   â”œâ”€â”€ reviewers/              # AI reviewer API integration
â”‚   â”œâ”€â”€ scorers/                # Performance evaluation
â”‚   â”œâ”€â”€ seeders/                # Error injection
â”‚   â””â”€â”€ utils/                  # Reporting and utilities
â””â”€â”€ results/                    # Generated after running
    â””â”€â”€ refereesim_YYYYMMDD_HHMMSS/
        â”œâ”€â”€ data/               # Generated papers
        â”œâ”€â”€ reviews/            # AI review results
        â””â”€â”€ reports/            # HTML reports and plots
```

## Example Output

```
ğŸ† FINAL RESULTS:
ğŸ“Š Papers Evaluated: 15
ğŸ¤– Models Tested: 11
ğŸ¯ Ground Truth Errors: 13
ğŸ“ Total Reviews: 165
ğŸ¥‡ Best Model: command-a-03-2025
   F1-Score: 0.847

ğŸ“Š CONFUSION MATRIX - ğŸ† WINNER: command-a-03-2025
==================================================
                 Predicted
               Error  No Error
Actual Error     38        7    (TP)  (FN)
   No Error      12      213    (FP)  (TN)
--------------------------------------------------
Accuracy:  0.930 (251/270)
Precision: 0.760 (38/50)
Recall:    0.844 (38/45)
F1-Score:  0.800
==================================================
```

## Stanford Agents4Science 2025

This platform was designed for the Stanford Agents4Science 2025 conference to evaluate AI reviewers on scientific paper error detection using real API calls and controlled synthetic datasets.